{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to train a recurrent neural network that classifies texts from the [20 newsgroups dataset](http://qwone.com/~jason/20Newsgroups/).\n",
    "\n",
    "The main purpose of this example is to illustrate how to use the pylat library to solve a complete text classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run init.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "We are going to use the scikit-learn.datasets module to load the texts and store them in the 'data' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['rec.autos', 'rec.sport.baseball', 'rec.sport.hockey']\n",
    "newsgroups_train = fetch_20newsgroups(data_home='./data', subset='train', categories=categories)\n",
    "\n",
    "texts = newsgroups_train.data\n",
    "labels = newsgroups_train.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we can see an example text from the dataset. We can see that there is some metadata that could be separated from the main text ('from', 'subject', 'nntp-posting-host', 'organization'...) and processed to potentially improve the performance of our final classifier. However, for the purpose of this example we will just work with the complete piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "In this section we are going to preprocess the texs before feeding them to our recurrent neural network. The following steps will be explained:\n",
    "* Preprocessing of the text: This includes tokenization, removal of stop words and lemmatization.\n",
    "* Training a Word2Vec model that maps tokens to a vector representation.\n",
    "* Using the trained Word2Vec model to convert each token in the texts to vectors that can be fed to the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "Pylat provides a TextPreprocessor class that takes care of tokenization, stop word removal and lemmatization. The constructor receives the following parameters:\n",
    "* remove_stop_words: boolean indicating if step words should be removed or not.\n",
    "* lemmatize: boolean indicating if the words should be lemmatized.\n",
    "* spacy_model_id: language to be used internally for tokenization of the text. Supported languages right now are 'en' for English and 'es' for Spanish. An English tokenizer is used by default.\n",
    "* additional_pipes: Iterable of callables that can be provided by the user to perform additional preprocessing steps.\n",
    "\n",
    "In the following cell we are going to create a TextPreprocessor object to tokenize and remove the stop words of our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylat.wrapper.transformer.text_preprocessor import TextPreprocessor\n",
    "\n",
    "preprocessor = TextPreprocessor(remove_stop_words=True, lemmatize=False)\n",
    "preprocessed_texts = preprocessor.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our texts preprocessed, we can feed them to Word2Vec to train the word embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec(preprocessed_texts, size=50, alpha=0.025, window=5, min_count=3,\n",
    "                     max_vocab_size=None, sample=0.001, seed=42, workers=3, iter=100, min_alpha=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model ready, pylat provides a class to transform the tokens to their vector representation. It also provides a SentencePadder transformer to make sure that all of our texts have the same size after being preprocessed and converted to a list of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylat.neuralnet.embeddings import Word2VecEmbedding\n",
    "from pylat.wrapper.transformer import SentencePadder, WordEmbeddingsTransformer\n",
    "\n",
    "w2v_embedding = Word2VecEmbedding(model=w2v_model)\n",
    "w2v_transformer = WordEmbeddingsTransformer(embeddings=w2v_embedding, to_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we put everything together to create our final data pipeline. This pylat can transform any text from our dataset to a vector representation that can be directly fed to our recurrent neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "w2v_data_pipeline = Pipeline(steps=[('preprocessing', preprocessor), \n",
    "                                    ('w2v', w2v_transformer), \n",
    "                                    ('padder', SentencePadder())])\n",
    "X_train_w2v = w2v_data_pipeline.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network creation\n",
    "\n",
    "After preprocessing the texts, we can move on to the creation of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a neural network with a specific architecture\n",
    "Pylat provides several classes to build and personalize the architecture of our neural network. In the package pylat.neuralnet.rnn we have available different implementations of network layers and cells to use in our classifier. In this example we are going to build a neural network with a recurrent layer and a dense layer.\n",
    "\n",
    "If we want to add more layers to the network, we just have to add additional RecurrentLayer or BidirectionalRecurrentLayer objects to the 'rnn_layers' list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylat.neuralnet import DenseLayer\n",
    "from pylat.neuralnet.rnn import BidirectionalRecurrentLayer, RecurrentLayer, \\\n",
    "    LSTMCellFactory, GRUCellFactory\n",
    "from pylat.wrapper.predictor import RNNWrapper\n",
    "\n",
    "rnn_layers = [RecurrentLayer(50, dropout_rate=0.35,\n",
    "                             cell_factory=GRUCellFactory(),\n",
    "                             cell_dropout=0.45)]\n",
    "fc_layers = [DenseLayer(20, activation='relu', dropout_rate=0.3)]\n",
    "\n",
    "rnn_w2v = RNNWrapper(embeddings=w2v_embedding, \n",
    "                     rnn_layers=rnn_layers, fc_layers=fc_layers,\n",
    "                     batch_size=50, early_stopping=False,\n",
    "                     learning_rate=1e-3, num_epochs=12,\n",
    "                     save_dir='results/rnn')\n",
    "rnn_w2v.fit(X_train_w2v, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to optimize the parameters, we can make use of the RandomizedSearchCV class provided by scikit-learn. We will first define a dict with the combination of parameters that we want to try, and later on we will pass our recurrent neural network to the RandomizedSearchCV constructor to find the best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_params = {\n",
    "    \"batch_size\": [25, 50, 75, 100],\n",
    "    \"num_epochs\": [10, 15, 20, 25, 30],\n",
    "    \"rnn_layers\": [(BidirectionalRecurrentLayer(50, dropout_rate=0.45, cell_factory=LSTMCellFactory(),\n",
    "                                          cell_dropout=0.55),)],\n",
    "    \"fc_layers\": [(DenseLayer(20, activation='relu', dropout_rate=0.3),)],\n",
    "    \"early_stopping\": [True, False],\n",
    "    \"learning_rate\": [0.001, 0.003, 0.01, 0.03]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_w2v = RNNWrapper(embeddings=w2v_embedding)\n",
    "rnn_w2v_grid = RandomizedSearchCV(rnn_w2v, rnn_params, n_iter=5, cv=4, scoring='f1_macro',\n",
    "                                  return_train_score=True, random_state=RANDOM_SEED)\n",
    "rnn_w2v_grid.fit(X_train_w2v, y_train)\n",
    "rnn_w2v_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "After training our models, we can evaluate their performance on the test dataset. First of all, we will load this set using the sklearn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test = fetch_20newsgroups(data_home='./data', subset='test', categories=categories)\n",
    "\n",
    "X_test = w2v_data_pipeline.fit_transform(newsgroups_test.data)\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make use of the predict function provided by the neural network to obtain the predictions for our test set. After obtaining the predictions we can make use of the sklearn.metrics module to compute common evaluation metrics such as the accuracy or f1_score.\n",
    "\n",
    "Pylat also provides additional functions that we can use to evaluate our models. In this case, we are going to compute the PPV (positive predicted value), the NPV (negative predicted value) and the Wilson Score interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pylat.evaluation import positive_predicted_value, \\\n",
    "                             negative_predicted_value, wilson_score_interval\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def measure_performance(model, X, y):\n",
    "    \"\"\"This method shows a summary of the performance of a model.\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    print(y_pred.shape)\n",
    "    print(y.shape)\n",
    "    f1 = f1_score(y, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    acc_interval = wilson_score_interval(1 - acc, len(y), 90)\n",
    "    ppv = positive_predicted_value(y, y_pred)\n",
    "    npv = negative_predicted_value(y, y_pred)\n",
    "    print('F1: {:.3f}, Accuracy: {:.3f} ± {:.3f}, PPV: {:.3f}, NPV: {:.3f}'.format(\n",
    "          f1, acc, acc_interval, ppv, npv))\n",
    "    \n",
    "measure_performance(rnn_w2v, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save our model to a file. This file could be loaded by other programs to obtain predictions from the network with new data:Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "def remove_dir(directory):\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory)\n",
    "\n",
    "\n",
    "def overwrite_dir(directory):\n",
    "    remove_dir(directory)\n",
    "    os.mkdir(directory)\n",
    "\n",
    "def save_neural_net(pipeline, neural_net, path, save_name):\n",
    "    \"\"\"Saves a recurrent neural network into a file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pipeline : sklearn.Pipeline\n",
    "        Data processing pipeline to be saved.\n",
    "    neural_net : :obj:`BaseNeuralNetwork`\n",
    "        Neural network that will be saved.\n",
    "    path : str\n",
    "        Directory where the model will be saved.\n",
    "    save_name : str\n",
    "        Name of the saved file.\n",
    "    \"\"\"\n",
    "    save_path = os.path.join(path, save_name)\n",
    "    overwrite_dir(save_path)\n",
    "    with open(os.path.join(save_path, 'pipeline.pk1'), 'wb') as f:\n",
    "        pickle.dump(pipeline, f)\n",
    "    model_dir = os.path.join(save_path, 'model')\n",
    "    remove_dir(model_dir)\n",
    "    neural_net.model.save(model_dir)\n",
    "\n",
    "save_neural_net(w2v_data_pipeline, rnn_w2v, 'classifiers', 'rnn_w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to load the model in another project, we can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_neural_net(save_path):\n",
    "    \"\"\"Loads a neural network model.\n",
    "    This method should be used to load neural networks that use TensorFlow as\n",
    "    the backend.\n",
    "    Parameters\n",
    "    ----------\n",
    "    save_path : str\n",
    "        Directory where the neural network has been saved.\n",
    "    Returns\n",
    "    -------\n",
    "    sklearn.Pipeline\n",
    "        Loaded scikit-learn pipeline. The last step of the pipeline corresponds\n",
    "        to the neural network, which has its weights restored from the save\n",
    "        file.\n",
    "    \"\"\"\n",
    "    pipeline_path = os.path.join(save_path, 'pipeline.pk1')\n",
    "    with open(pipeline_path, 'rb') as f:\n",
    "        pipe = pickle.load(f)\n",
    "    model_path = os.path.join(save_path, 'model')\n",
    "    embeddings = pipe.steps[1][1].embeddings\n",
    "    rnn = RNNWrapper(rnn_layers=[DenseLayer(10)],\n",
    "                     fc_layers=[RecurrentLayer(10)], embeddings=embeddings)\n",
    "    rnn.model.restore(model_path)\n",
    "    pipe.steps.append(('rnn', rnn))\n",
    "    return pipe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
